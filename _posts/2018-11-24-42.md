---
title: Sequence to Sequence Model
categories: Deep-Learning Recurrent-Neural-Networks
header:
  teaser: /assets/teasers/7.jpg
---



Original Source: https://www.coursera.org/specializations/deep-learning



Sequence to sequence model is a many-to-many RNN architecture where we encode a sequence and decode it as a sequence.

![sequence to sequence model](https://lh3.googleusercontent.com/c9-mH6_LFdIi_veanlmLQ970pRymFOVjGLwIsxuyB8N_F5anuaLco2n5-GY653XYvVdg1aW4_FaYeIFDuWNRDEFpBwSDTLTwM53kvYLK8ds-uPM0H0wenajbO5P9ugydZHUbnqYmyQ=w2400)

# Examples of Sequence Models

Machine Translation is an example of sequnce to sequnce models. First, you input original sentence to an RNN(maybe a BRNN with LSTM). Then feed the output as $x^{\<1\>}$ of the second RNN. In the second RNN, predicted word from $\hat{y}^{\<t\>}$ becomes $x^{\<t+1\>}$ like in language model.

![machine translation](https://lh3.googleusercontent.com/aUML2CSEUXHu0j1dziNZmPsIuZ2cienK_-CEgjnoS0ovTHv_CqGQoMTwGOVjM4LPkqhsXCrQL61DEgH9U4zteScttXLWTmg6BWD4o2vDvCyV-f-PhaSxYlcpiOJDjP3WYkGhcA0Ojw=w2400)

Image Captioning is another example of encoding-decoding model (similar to sequence to sequence model, precisely an image to sequence model). We first encode the image with CNN, then with the encoded vector as an input train a language model like RNN.

![image captioning](https://lh3.googleusercontent.com/aJSlIBSxvxiBRdbk_cJ_2XXKLFDWUgSKDLxJHybQ70Crv958ZVvPaJRcygTQtod_sg1_96H3OxA1bAuo74bqm4S4jDe0v8ZzhFp-slPGOCCV6uh4W2mvjghnY-26FWXII-e7rHX9RA=w2400)

# Difference from Language Model

Sequence to sequence model is similar to language model but has two differences.

First, sequence to sequence model has an 'encoding' part. In language model, we've initialized $x^{\<1\>}$ with 0 vector, whereas in sequence to sequence model, we use output of the encoding RNN as $x^{\<1\>}$ in the decoding RNN.

Second, sequence to sequence model picks the 'most' likely sentence. In language model, we've randomly picked a word from distribution of $\hat{y}^{\<t\>}$ to feed into the next sequential layer. However, in sequence to sequence model, we want to pick the most likely sentence. So our objective is as follows;

$$
\max_y p(y^{<1>},...,y^{<T_y>}|x^{<1>})
$$

where $T_y$ is the length of the output sequence.

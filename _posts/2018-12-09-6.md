---
title: UCB1
categories: Reinforcement-Learning
header:
  teaser: /assets/teasers/8.jpg
---

UCB1 is yet another method to solve explore-exploit.

We first calculate UCB1 value for each bandit at every timestep.

$$
X_{UCB-j} = \bar{X}_j + \sqrt{2 \frac{\ln{N}}{N_j}}
$$

where $N_j$ is number of times you tried $j$ th bandit, $N$ is total number of trials you've done, and $\bar{X}_j$ is the mean of rewards you got from $j$ the bandit machine.
Then at every timestep, we choose the bandit with the highest $X_{UCB-j}$ .

But how does it work?

If you've tried $j$ th bandit for only small number of times, so when $N_j$ is small relative to $N$ , $X_{UCB-j}$ would be large, so you will be likely to try $j$ th bandit at next timestep. If you've tried $j$ th bandit for lot of times, so when $N_j$ is large relative to $N$ , $X_{UCB-j}$ would be small, so you will be less likely to try $j$ th bandit at next timestep. This is how UCB1 algorithm cares exploration.

Also, $\ln{N}$ grows more slowly than $N_j$ , so eventually you will select bandit based on mostly $\bar{X}_j$ ; that is, we converge to purely greedy strategy. This is how UCB1 algorithm cares exploitation.


```python
import numpy as np
import matplotlib.pyplot as plt
```


```python
class Bandit:

    def __init__(self, m):
        self.m = m
        # set initial reward mean to a ceiling (we know that every bandit reward is lower than 10)
        self.mean = 0
        self.N = 0

    def pull(self):
        return np.random.randn() + self.m

    def update(self, x):
        self.N += 1
        self.mean = (1-1/self.N)*self.mean + 1/self.N*x
```


```python
def ucb(mean, n, nj):
    if nj == 0:
        return float('inf')
    return mean + np.sqrt(2*np.log(n) / nj)
```


```python
def run_experiment(m1, m2, m3, N):

    data = np.empty(N)
    bandits = [Bandit(m1), Bandit(m2), Bandit(m3)]

    for i in range(N):
        j = np.argmax([ucb(b.mean, i+1, b.N) for b in bandits])
        x = bandits[j].pull()
        bandits[j].update(x)
        data[i] = x

    cumulative_average = np.cumsum(data) / (np.arange(N)+1)

    print('mean reward for each choice after {} trials with UCB1'.format(N))
    print([b.mean for b in bandits])

    return cumulative_average
```


```python
# for comparison between epsilon-greedy method and optimistic-initial-values method

class BanditEps:

    def __init__(self, m):
        self.m = m
        self.mean = 0
        self.N = 0

    def pull(self):
        return np.random.randn() + self.m

    def update(self, x):
        self.N += 1
        self.mean = (1-1/self.N)*self.mean + 1/self.N*x

def run_experiment_eps(m1, m2, m3, eps, N):
    bandits = [BanditEps(m1), BanditEps(m2), BanditEps(m3)]
    data = np.empty(N)
    for i in range(N):
        p = np.random.random()
        if p < eps:
            j = np.random.choice(3)
        else:
            j = np.argmax([bandit.mean for bandit in bandits])
        x = bandits[j].pull()
        bandits[j].update(x)
        data[i] = x
    cumulative_average = np.cumsum(data) / (np.arange(N)+1)

    print('mean reward for each choice after {} trials with epsilon={}'.format(N, eps))
    print([b.mean for b in bandits])

    return cumulative_average
```


```python
if __name__ == '__main__':

    ucb1 = run_experiment(1.0, 2.0, 3.0, 100000)
    eps = run_experiment_eps(1.0, 2.0, 3.0, 0.1, 100000)

    # log scale plot
    plt.figure(figsize=(20,10))
    plt.plot(ucb1, label='ucb1')
    plt.plot(eps, label='eps = 0.1')
    plt.legend()
    plt.xscale('log')
    plt.show()
```

    mean reward for each choice after 100000 trials with UCB1
    [1.2990129432162183, 2.1063998932557393, 3.0025851412596474]
    mean reward for each choice after 100000 trials with epsilon=0.1
    [1.0031120521765773, 1.977229283874808, 2.997791116809667]



![png](https://lh3.googleusercontent.com/FhQs68Ewd0J0YuI26rbsUI99LBKadNMcrQRayChRnXFj1C6qSV4UXys_h7uHuMsD3phUwziB4M3jq04qt_Ckf3zzMG7aEaY-KqOeEldU5jrZG5qDUyLf77oracFngmH2bykXk7q8AA=w2400)

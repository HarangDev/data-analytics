---
title: Explore-Exploit Dilemma
categories: Reinforcement-Learning
header:
  teaser: /assets/teasers/8.jpg
---

The multi-armed bandit problem is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice.

The name comes from imagining a gambler at a row of slot machines, who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.

Here, dilemma of explore-exploit rises. You can try all of the machines for a long time before choosing the best slot machine you will play afterwards. You will waste lots of time and money when exploring slot machines, but with so much data you will be more likely to exploit best slot machine. On the other hand, you can try machines only for several times and choose the machine. Cost from exploring will be low, but the probability that you will choose the best one will be lower than the first case.

In summary, in multi-armed bandit problem you need to balance explore(collecting data) and exploit(playing 'best-so-far' machine).

## 1. Traditional A/B Testing

You predetermine number of times you need to play in order to establish statistical significance. If you set N=1000, you should explore 1000 times even if you feel confident after only 500 trials.

Thus, we can say that A/B testing emphasizes explore over exploit.

## 2. Human Emotion

Suppose you are playing in a real casino, and get 2/3 on one bandit and 0/3 on the other. You will likely play the first bandit afterwards. However, actually 3 is a small sample size to know bandit machine probabilities.

Following human emotion is emphasizing exploit over explore.

## 3. Epsilon-Greedy

Epsilon-Greedy method is a solution to the explore-exploit dilemma.

First, choose a small number $\epsilon$ as a probability of exploration. Typical values of $\epsilon$ are 0.05 and 0.1.

Then, pick a random number from 0 to 1. If that number is smaller than $\epsilon$, we pull random arm (explore), and we pull currently best arm (exploit) otherwise.

In the long run, epsilon-greedy allows us to explore each arm an infinite number of times. So we will eventually get to the true best arm. However, with this method, we get to a point where we explore when we don't need to. We may solve this problem by decreasing $\epsilon$ for each trial.

---
title: Activation Functions
categories: Deep-Learning
header:
  teaser: /assets/teasers/7.jpg
---

# Why Use Non-Linear Activation Function?

The reason we used hidden layers was to represent complex, non-linear model. Non-linear activation function adds this complexity to the model.

If we don't use activation or use linear activation function, our final model can be represented with a single linear regression or logistic regression.

# Pros and Cons of Activation Function

![activation functions](https://lh3.googleusercontent.com/7GQl9SapuXE5jVz0Mj3ZEwytYc82uDq4zGa7dT2f1SIyJzsQPImHhKuk7nefBtp8PV6e-vt5MZvv-gObp2Loi3H_y8pu2YGzbnU9fP2otgJRInYjwlKGIGdnVuo26AN77HcAna7AuA=w2400)

* Tanh function is always faster than sigmoid function.
* With sigmoid and tanh, when absolute value of z becomes big, slope of function converges to 0, so gradient descent becomes slower.
* Most times, we use ReLU which is faster than tanh
* Try using Leaky ReLU
* Sigmoid is commonly used as an activation function of output layer in binary classification

---
title: Multivariate Linear Regression
categories: Machine-Learning
header:
  teaser: /assets/teasers/6.jpg
---

 Multivariate linear regression is a linear regression with multiple variables.

# Notations

$m$: the number of training examples

$n$: the number of features

$k$: the number of output classes

$X$: an input matrix, where each row represents each training example, and each column represents each feature. Note that first column $X_1$ is a vector composed only of 1s.

$X^{(i)}$: the row vector of all the feature inputs of the ith training example

$X_j$: the column vector of all the jth feature of training examples.

$X_j^{(i)}$: value of feature j in the ith training example.

$y$: an output column vector, where each row represents each training example.

$\theta$: column vector of weights

# Hypothesis

First we will assume there is only one training example $x$.

$$h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n$$

Below equation is the same as the above.

$$\begin{align*}h_\theta(x) =\begin{bmatrix}x_0 \hspace{2em}  x_1 \hspace{2em}  ...  \hspace{2em}  x_n\end{bmatrix}\begin{bmatrix}\theta_0 \newline \theta_1 \newline \vdots \newline \theta_n\end{bmatrix}= x\theta\end{align*}$$

### Vectorized Version

We have implemented cost function for every training examples. By 'vectorizing' this implementation, we can deal with those training examples at the same time. It saves tremendous amount of time and resource when it comes to computing with computer.

Think of vectorizing as stacking training examples from top to bottom of the input. So the training examples are stored row-wise in matrix X. Thetas are also stacked vertically in a vector.

$$\begin{align*}X = \begin{bmatrix}X^{(1)}_0 & X^{(1)}_1  \newline X^{(2)}_0 & X^{(2)}_1  \newline X^{(3)}_0 & X^{(3)}_1 \end{bmatrix}&,\theta = \begin{bmatrix}\theta_0 \newline \theta_1 \newline\end{bmatrix}\end{align*}$$

Then the generalized hypothesis is as follows.

$$h_\theta(X) = X \theta$$

# Cost Function

$$J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (X^{(i)}) - y^{(i)} \right)^2$$

### Vectorized Version

$$J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})$$

where $\vec{y}$ denotes the vector of all y values.

# Gradient Descent

$$\begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(X^{(i)}) - y^{(i)}) \cdot X_j^{(i)} \;  & \text{for j := 0..n}\newline \rbrace\end{align*}$$

### Vectorized Version

Vectorized version of gradient descent is basically this.

$$\theta := \theta - \alpha \nabla J(\theta)$$

where

$$\nabla J(\theta)  = \begin{bmatrix}\frac{\partial J(\theta)}{\partial \theta_0}   \newline \frac{\partial J(\theta)}{\partial \theta_1}   \newline \vdots   \newline \frac{\partial J(\theta)}{\partial \theta_n} \end{bmatrix}$$

Like we've seen in univariate linear regression, we calculate derivative of each theta as follows.

$$\begin{align*}
\; &\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum\limits_{i=1}^{m}   X_j^{(i)} \cdot \left(h_\theta(X^{(i)}) - y^{(i)}  \right)
\end{align*}$$

We can vectorize above equation.

$$\begin{align*}\; &\frac{\partial J(\theta)}{\partial \theta_j} = \frac1m  {X_j}^T (X\theta - \vec{y})\end{align*}$$

Then

$$\begin{align*}\; &\nabla J(\theta) = \frac 1m X^{T} (X\theta - \vec{y}) \newline\end{align*}$$

In conclusion, vectorized gradient descent rule is:

$$\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})$$

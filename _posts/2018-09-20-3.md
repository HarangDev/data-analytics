---
title: Multivariate Linear Regression
categories: Machine-Learning
header:
  teaser: /assets/teasers/6.jpg
---

 Multivariate linear regression is a linear regression with multiple variables.

# Notations

$x_j^{(i)}$: value of feature j in the ith training example

$x^{(i)}$: the column vector of all the feature inputs of the ith training example

$m$: the number of training examples

$n$: the number of features

# Hypothesis

First we will assume there is only one training example.

$$h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n$$

Below equation is the vectorized version of the above.

$$\begin{align*}h_\theta(x) =\begin{bmatrix}\theta_0 \hspace{2em}  \theta_1 \hspace{2em}  ...  \hspace{2em}  \theta_n\end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T x\end{align*}$$

When there are multiple training examples, we stack them left to right. So the training examples are stored row-wise in matrix X. Also, thetas are stacked vertically in a vector.

$$\begin{align*}X = \begin{bmatrix}x^{(1)}_0 & x^{(1)}_1  \newline x^{(2)}_0 & x^{(2)}_1  \newline x^{(3)}_0 & x^{(3)}_1 \end{bmatrix}&,\theta = \begin{bmatrix}\theta_0 \newline \theta_1 \newline\end{bmatrix}\end{align*}$$

Then the generalized hypothesis is as follows.

$$h_\theta(X) = X \theta$$

# Cost Function

$$J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (x^{(i)}) - y^{(i)} \right)^2$$

### Vectorized Version

$$J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})$$

where $\vec{y}$ denotes the vector of all y values.

# Gradient Descent

$$\begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \;  & \text{for j := 0..n}\newline \rbrace\end{align*}$$

### Vectorized Version

Vectorized version of gradient descent is basically this.

$$\theta := \theta - \alpha \nabla J(\theta)$$

where

$$\nabla J(\theta)  = \begin{bmatrix}\frac{\partial J(\theta)}{\partial \theta_0}   \newline \frac{\partial J(\theta)}{\partial \theta_1}   \newline \vdots   \newline \frac{\partial J(\theta)}{\partial \theta_n} \end{bmatrix}$$

Like we've seen in univariate linear regression, we calculate derivative of each theta as follows.

$$\begin{align*}
\; &\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum\limits_{i=1}^{m}   x_j^{(i)} \cdot \left(h_\theta(x^{(i)}) - y^{(i)}  \right)
\end{align*}$$

We can vectorize above equation.

$$\begin{align*}\; &\frac{\partial J(\theta)}{\partial \theta_j} = \frac1m  \vec{x_j}^{T} (X\theta - \vec{y})\end{align*}$$

Then

$$\begin{align*}\; &\nabla J(\theta) = \frac 1m X^{T} (X\theta - \vec{y}) \newline\end{align*}$$

In conclusion, vectorized gradient descent rule is:

$$\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})$$

---
title: Introduction to Machine Learning
categories: Applied-Data-Science-with-Python Applied-Machine-Learning-in-Python
header:
  teaser: /assets/teasers/3.jpg
---

# What is Machine Learning?

***The study of computer programs (algorithms) that can learn by example***

### ML Algorithms learn rules from labelled examples.
* A set of labelled examples used for learning is called **training data**.
* The learned rules should also be able to generalize to correctly recognize or predict new examples not in the training set.

### Machine Learning brings together statistics, computer science, and more, depending on the specific goal.

### Examples of Machine Learning
* Fraud detection <br>
    Training Data: Credit card transaction history <br>
    Label: Whether each transaction was fraud.<br>
    Develop model that predicts which transactions are fraudulent.
* Web search: query spell-checking, result ranking, content classification and selection, advertising placement.
* Speech Recognition
* eCommerce: Product recommendations
* Email spam filtering
* Health applications: Drup design and discovery
* Education: Automated essay scoring

# Categories of Machine Learning

## A. Supervised machine learning

Model learns to predict target values from labelled data. The example 'Fraud detection' above is a supervised classification machine learning task.

### 1. Classification

Target values are discrete classes

![supervised learning classificaiton](https://lh3.googleusercontent.com/tRhHQTKzNfr-eg1sSdaIxTbU7jTzmKamvAZbK393ftcBjP5Kpqevqv5D3JbIPdcIzyrScUQ09jk90LCO8svgRBuj9j-ytJexXMf7l9oPNbY3-SJbmUW_DuXBMQfio5sYn6KJzeJSuQ=w2400)

### 2. Regression

Target values are continuous values

## B. Unsupervised machine learning

Find structure in unlabeled data

1. Clustering <br>
    ex) Finding clusters of similar users
1. Unsupervised outlier detection <br>
    ex) Detecting abnormal server access patterns

![unsupervised learning classification](https://lh3.googleusercontent.com/Mf9iadfjINBQkQbktbiXZODoC_UxGWFZYN5VrvdFuD86GmGNGvTIn_t3rpH6RWR86y3RCpekDQ1FcHQ-zBwCpvP0T3AHkgU4ETNeD0P9Txhz8uA3A9I65lK0NDVo9lLukiLUK-n5YQ=w2400)

# Basic Machine Learning Workflow

![basic machine learning workflow](https://lh3.googleusercontent.com/tQQFgFiJU3U_rp9Xg4zClZ-zsc9MmBdPFLsbv59Rg2lwsQ7Mb4s66tKOILglrrQlES4njs0BTyxB_b8ky5cXnb3AcefqdbikSrH9g1fikAppAui0Bjc_mRTI4W0sQ3aAL1opWldoAg=w2400)

### 1. Representation
Extract and select object features

![feature extractions](https://lh3.googleusercontent.com/_r1k22RysdyKWVtfOkwdPOK0xUf3Xt9thmDksriToFO9N51JPIvzbXyptx5UCPY1ekga-Rz_g8f9p0gD5p1QCcrphU9MOkqXT7Qd38vjDH1rDAuTlfyQ7sOBJHyZk_6o6Tqo38FhvA=w2400)

### 2. Train models
Fit the estimator to the data

### 3. Evaluation
Does this feature and estimator predict successfully?

### 4. Feature and model refinement

# Python Tools for Machine Learning

1. scikit-learn: Python Machine Learning Library
2. NumPy: Scientific computing library
3. Pandas: Data manipulation
4. matplotlib: plotting library


# k-Nearest Neighbor (k-NN) Classifier

1. Find the most similar instances (let's call them X_NN) to x_test that are in X_train.
2. Get the labels y_NN for the instances in X_NN
3. Predict the label for x_test by combining the labels y_NN (e.g. simple majority vote)

## k-NN needs four things specified

1. A distance metric <br>
    Typically Euclidean (Minkowski with p = 2)
2. How many 'nearest' neighbors to look at? <br>
    e.g. five
3. Optional weighting function on the neighbor points <br>
    Typically ignored
4. How to aggregate the classes of neighbor points <br>
    Typically Simple majority vote (Class with the most representatives among nearest neighbors)

## Visual explaining effect of 'k'

![k-NN](https://lh3.googleusercontent.com/f3dGiSx3gqeLbOCJO1skICiecLKZRmtSerJs84lh5Qze3fa8nqnCEVXiZbjUodzTjzwBTjKAV9r00bgHCSydcBhpcdDF8W7BWH7OcI7nbupAwE_GNw6vXi3lbsLLSOU86epZg8h1TQ=w2400)

# Example Machine Learning Problem with k-NN

### Import required modules and load data file

The input data as a table

![jpg](https://lh3.googleusercontent.com/I2Lrn8YiXmr5YfbIcxOT1-ruXbNHJR8o6RxZFwsnaGMDwJqX8achnoYy7q_srDzQEErTkBLdsXbKrZfrzfAMOZIB1JjAgW1NRO_hPO19lYNge_ZrM_QVN_lCvRkI0w3SKMMH0V8Q2A=w2400)


```python
%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

# set default figure size to (14, 8)
plt.rcParams['figure.figsize'] = (14.0, 8.0)

fruits = pd.read_table('fruit_data_with_colors.txt')
```


```python
fruits.shape
```




    (59, 7)




```python
fruits.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fruit_label</th>
      <th>fruit_name</th>
      <th>fruit_subtype</th>
      <th>mass</th>
      <th>width</th>
      <th>height</th>
      <th>color_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>192</td>
      <td>8.4</td>
      <td>7.3</td>
      <td>0.55</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>180</td>
      <td>8.0</td>
      <td>6.8</td>
      <td>0.59</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>apple</td>
      <td>granny_smith</td>
      <td>176</td>
      <td>7.4</td>
      <td>7.2</td>
      <td>0.60</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>mandarin</td>
      <td>mandarin</td>
      <td>86</td>
      <td>6.2</td>
      <td>4.7</td>
      <td>0.80</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>mandarin</td>
      <td>mandarin</td>
      <td>84</td>
      <td>6.0</td>
      <td>4.6</td>
      <td>0.79</td>
    </tr>
  </tbody>
</table>
</div>




```python
# create a mapping from fruit label value to fruit name to make results easier to interpret
lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))   
lookup_fruit_name
```




    {1: 'apple', 2: 'mandarin', 3: 'orange', 4: 'lemon'}



### Create train-test split

If we use whole data as training set, our model can overfit to training set so it might not generalize to real world cases. Thus, we evaluate our model with hold-out validation set or development set and tune our hyperparmeters(e.g. value k in k-NN) based this evaluation. <br> `sklearn.model_selection.train_test_split` splits data into train set and test(validation, development) set.

![jpg](https://lh3.googleusercontent.com/iNPKT18Wt9f_GZOq18Uvpx8i72rsan7RVkKC1yizfOq6bZYaSZBUouzPTPcwpVfTjT42KPTJf_HJA3HLa9JYwLLo_GvCJcSZ5P08HDpSMDKdMpyCAxeTNYOgGnRjVesl-A-d97-Hxg=w2400)


```python
# For this example, we use the mass, width, and height features of each fruit instance
X = fruits[['mass', 'width', 'height', 'color_score']]
y = fruits['fruit_label']

# default is 75% / 25% train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
```


```python
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
```

    (44, 4)
    (15, 4)
    (44,)
    (15,)


### Examining the data

**Reasons why looking at the data initially is important**
* Inspecting feature values may help identify what cleaning or preprocessing still needs to be done once you can see the range or distribution of values that is typical for each attribute.
* You might notice missing or noisy data, or inconsistencies such as the wrong data type being used for a column, incorrect units of measurements for a particular column, or that there aren't enough examples of a particular class.
* You may realize that your problem is actually solvable without machine learning.

*Example of incorrect or missing feature values*

![jpg](https://lh3.googleusercontent.com/6IfdPKysWS2rAsXVWeamS2iTlqo_5rjJy8x1uqnn67acQud_hpVp_VDuf18y29v6A9XTDuxdwqW6UrOuYagx8dPb8FKEEDnDGiayrtRcJAh5gGQ3njK93FYxQL7LFsOQmB5OguFWyA=w2400)

**Plotting pairwise feature scatterplot** <br>
It visualizes the data using all possible pairs of features, with one scatterplot per feature pair, and histograms for each feature along the diagonal.


```python
import seaborn as sns
sns.set()
sns.pairplot(fruits.iloc[:, 1:], hue='fruit_name')
```




    <seaborn.axisgrid.PairGrid at 0x179d2a53748>




![png](https://lh3.googleusercontent.com/jXzkeDeGIBnRT6rEZSWFW09VdoT0SMyYBa_8CM2tVMZj7EfYSDNWgXWAIM2F-HfJwXpxgpIHrLgWCEfD0fRgB6EZZFKQWSCxhb612eIHKcXR_C9sTbUCZPRyRW0EyGvqhh-ZKmfjVQ=w2400)


**A three-dimensional feature scatterplot**


```python
# plotting a 3D scatter plot
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker = 'o', s=100)
ax.set_xlabel('width')
ax.set_ylabel('height')

ax.set_zlabel('color_score')
plt.show()
```


![png](https://lh3.googleusercontent.com/mQxts9q3FkclZ0JIE6lHUB7zWtQAWqM1bqsPZ9zjiZAyI7I5BPM4dIXJpv0K_Fyo5mtSTzGWcKU8ucFPM_yL7AiZE6B5kNiCfqgFukEPteGe3rp_zscGm5_O_x3fdsZBroQspfFgGw=w2400)


### Create classifier object


```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 5)
```

### Train the classifier (fit the estimator) using the training data


```python
knn.fit(X_train, y_train)
```




    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
               metric_params=None, n_jobs=1, n_neighbors=5, p=2,
               weights='uniform')



### Estimate the accuracy of the classifier on future data, using the test data


```python
knn.score(X_test, y_test)
```




    0.5333333333333333



### Use the trained k-NN classifier model to classify new, previously unseen objects


```python
# first example: a small fruit with mass 20g, width 4.3 cm, height 5.5 cm
fruit_prediction = knn.predict([[20, 4.3, 5.5, 0.5]])
lookup_fruit_name[fruit_prediction[0]]
```




    'mandarin'




```python
# second example: a larger, elongated fruit with mass 100g, width 6.3 cm, height 8.5 cm
fruit_prediction = knn.predict([[100, 6.3, 8.5, 0.5]])
lookup_fruit_name[fruit_prediction[0]]
```




    'lemon'



### How sensitive is k-NN classification accuracy to the choice of the 'k' parameter?


```python
k_range = range(1,20)
scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors = k)
    knn.fit(X_train, y_train)
    scores.append(knn.score(X_test, y_test))

plt.figure()
plt.xlabel('k')
plt.ylabel('accuracy')
plt.scatter(k_range, scores)
plt.xticks([0,5,10,15,20])
plt.show()
```


![png](https://lh3.googleusercontent.com/VzLGrKl_pXqZq5v3qLSZ6CIFmMFTnkD2_dx1YmxNZaONBcp15cJIN7QsK6Y3fvYXXwJZAbE8ZSs3bUjzz4a3jqnJit4wdnjRtLys-Ya0042hdQ0m6x4L4XIfYxDGI9QL4wVyvAp4SQ=w2400)

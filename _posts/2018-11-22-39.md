---
title: Learning Word Embedding - Word2Vec, Negative Sampling, GloVE(Global Vectors for word representation)
categories: Deep-Learning Recurrent-Neural-Networks
header:
  teaser: /assets/teasers/7.jpg
---

How do we learn embedding matrix?

Basically, we randomly initialize embedding matrix of shape $k \times n$ $n$ is the size of the vocabulary and $k$ is the number of features. Then, with a large text corpus as training set, we define loss based on how frequently a 'target' word appears together with 'context' words in each example. Finally we do gradient descent(or other optimizing algorithms) to optimize embedding matrix $E$.

There are several algorithms that define loss in different ways.

# Word2Vec

*https://arxiv.org/abs/1301.3781*

First, pick one 'content' word then pick one 'target' word that appears near the content word in training corpus. This forms one training example (also called a 'skip-gram').

Now we construct a simple neural network with input 'content' and label 'target'.

For single training example, note that $\hat{y}_i$ which is the $i$ th element of our prediction $\hat{y}$, would be a probability that the target word belongs to $i$ th vocabulary. Also our label $y$ would be a one-hot representation of our target word (a vector of 0s except $t$ th element which is 1).

Let's see how a single training example passes through our network. The parameters that we should optimize are embedding matrix $E$ and $\theta_i$ s ( $i=1,...,n$ ), which are the weight vectors associated with output $\hat{y}_i$.

$$
e_c = Eo_c
$$

where $o_c$ is a one-hot representation of a content word.

Pass this into a layer with softmax activation.

$$
\hat{y}_i = p(i|c) =  \frac{exp(\theta_i^Te_c)}{\sum_{j=1}^n exp(\theta_j^Te_c)}
$$

for $i=1,...,n$

$$
L(\hat{y}, y) = -\sum_{i=1}^n y_i \log(\hat{y}_i)
$$

Now let's define cost that considers all of the training examples.

$$
C = \frac{1}{m}\sum_{k=1}^m L
$$
With this cost, we do gradient descent to optimize embedding matrix $E$ and $\theta$s.

# Negative Sampling

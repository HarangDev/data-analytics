---
title: Language Model with RNN
categories: Deep-Learning Recurrent-Neural-Networks
header:
  teaser: /assets/teasers/7.jpg
---

# What is a Language Model?

Given a sequence of words $y^{<1>},...,y^{<T_y>}$ , a language model computes how likely this sequence would exist; $p(y^{<1>},...,y^{<T_y>})$ .

For example, a language model would say that p(pair of shoes) is bigger than p(pear of shoes).

**How do we build Language Model with RNN?**

# Collect training data

Our training set will be a large corpus(set) of english text.

# Tokenization

First, we tokenize words into vectors. Usually, we add \<EOS\>(end of sentence) and \<UNK\>(unknown) tokens to our vocabulary. So if there are $n$ words in our vocabulary, each of the word vector would have $n+2$ elements.

Also, we can decide on whether to get rid of punctuations.

For example, let's say we want to tokenize sentence 'I like Sarah.' and don't consider punctuations. Sarah isn't in our vocabulary. Then our sequence will be $[y^{<I>}, y^{<like>}, y^{<UNK>}, y^{<EOS>}]$ , where each $y$ will be a vector representation of each word.

# RNN Model

![RNN Language Model](https://lh3.googleusercontent.com/BICtSJ7n6fF0Fv_rsHfVdgntAIbeeT4j-geZh2EZQsG2WxMgMczpEyu-kS-lB21-l1nUP3ZjAcmGV3A0xDbB8zMYmohUtLXmgsvvE_tjTbcbPz3TF0x3TUX_m5X3wEyFMWJzIkhFYQ=w2400)

First, set $x^{<1>}$ and $a^{<0>}$ a vector composed of 0s. Then, we pass it to our RNN architecture.

$y^{<t>}$ will become $x^{<t+1>}$ and feeded into the next layer.

$y^{<t>}$ , which is a softmax of $W_{ya}a^{<t>}+b_y$ , will be a probability distribution that a word will appear in $t$th sequence, given previous words. For example, $\hat{y}^{<2>}$ actually means $p(y^{<2>}\|y^{<1>})$ (probability words will appear in 2nd place given 1st probability distribution).

From this RNN model, we can know two things.

First, we can know probability a input sentence exist. We can multiply all of the output $\hat{y}$ s to compute that probability.

Second, we can know probability each word will come next given initial $t-1$ sequence of words. That is particularly $\hat{y}^{<t>}$ . For example, we can know that a word 'school' has higher probability than a word 'snake', when predicting what word will come next after 'I went to'.

# Loss Function

$$
L^{<t>}=\sum_i y_i^{<t>}\log‚Å°\hat{y}_i^{<t>}
$$

$$
C = \sum_t L^{<t>}
$$

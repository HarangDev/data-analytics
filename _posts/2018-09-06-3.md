---
title: Statistical Analysis in Python (Distribution, Hypothesis Testing) [Introduction to Data Science in Python - 3]
categories: Applied-Data-Science-with-Python Introduction-to-Data-Science-in-Python
header:
  teaser: /assets/teasers/1.jpg
---

# Distributions

**Definition**:Set of all possible random variables

## Types of Distributions

### Uniform Distribution

![uniform distribution](https://lh3.googleusercontent.com/DL3XDI-0smybHGTsXx6q30QSvInzc7ZaMzCv63mjqBMbjeYL-t1d4C8fZ59e6qZKx-sW3yvDegt5leTQVH8bGQ-OR-pKt0lZYyVsjf5RvcpJEXAlycDRZbROs0Kb2LZI6ErO9nrmNQ=w2400)

### Normal (Gaussian) Distribution

![normal distribution](https://lh3.googleusercontent.com/oCBO7_VNNZMqVptGlsMdwkekecC0J5jeKIpebqjeq1hf2JeHzHKQzJl-XdkWzipHZwu0asHJQvEkiuINZGedL1IlePNh_FluyFrQ_tzpOJ6fnU2xV-H5u47Pg6Dygfxhv5Ln0dbPQQ=w2400)

Formula for standard deviation: $\sqrt{\frac{1}{N} \sum_{i=1}^N (x_i - \overline{x})^2}$

### Chi Squared Distribution

![chi squared distribution](https://lh3.googleusercontent.com/oENXjlg3-1aFDedJK5CGHbGLGJj6lHr8Eu1e2uOxWQYKvmhEQiJFO1EjPKWvRdiswSz-tLvFftAmXPEpmhHIEQLzGB0zuaK7lko81Ii8rTme4TKDUvvWczxbCrwBPJIuZEJ6gGCwVg=w2400)

**Skewness & Kurtosis**

![skewness and kurtosis](https://lh3.googleusercontent.com/vF5l3Ab9f1FKs-kcuZBpE8Yh5ZMUSSF_E7gdXvf4YgM30xKgmz0WbhJQTFOimdzMJXLMMp_K97D6nl3WgYW7Vsj5Pb8kb8tehrrodU1V--7ELOUTmBV9rFvvcZgamdaeLHti_6uJsg=w2400)

Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.<br>
When 'Degrees of Freedom' gets smaller, the graph gets skewed to left.

Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.

### Bimodal distributions

![bimodal distributions](https://lh3.googleusercontent.com/_Y761ZK7h9jrmsdZQpEKhatt5eFNytz1efgJ6jHe1seWO94Zp-FbgSItD9wjg95_PoQt1uj_4GRYfqWyjvKFbdLFO05gZ3AZ9fc0xHmc00yIFbfHgD_LjuwQaGqJ_GSrAE069QC2Xw=w2400)

# Distributions in Pandas


```python
import pandas as pd
import numpy as np
```


```python
np.random.binomial(1, 0.5)
```




    1




```python
# Flip 1000 coins and check number of heads. Do this 10 times.
np.random.binomial(1000, 0.5, 10)
```




    array([502, 511, 516, 494, 527, 512, 529, 521, 495, 487])




```python
# sample a number from uniform distribution between 0 and 1
np.random.uniform(0, 1)
```




    0.2828033347414428




```python
#sample a number from normald distribution of mean=0, std=1
np.random.normal(loc=0, scale=1)
```




    -1.3560071953448096




```python
distribution = np.random.normal(loc=0, scale=1, size=1000)
distribution.std()
```




    0.9831044712693359




```python
import scipy.stats as stats
stats.kurtosis(distribution)
```




    0.025384635432275093




```python
stats.skew(distribution)
```




    0.055923877445990366




```python
chi_squared_df2 = np.random.chisquare(2, size=10000)
stats.skew(chi_squared_df2)
```




    2.0018163864318907




```python
chi_squared_df5 = np.random.chisquare(5, size=10000)
stats.skew(chi_squared_df5)
```




    1.3161735810606243




```python
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

plt.figure(figsize=(14,8))
plt.hist([chi_squared_df2,chi_squared_df5], bins=100, label=['2 degrees of freedom','5 degrees of freedom'], histtype='stepfilled', alpha=0.6)
plt.legend()
```




    <matplotlib.legend.Legend at 0x17d0be8e7f0>




![png](https://lh3.googleusercontent.com/WdT-zcBM2wg9BPvt633IeYi-6w_OSn-ldPn4C6jwsUxgN7tDLQsF03etQX88UBrNDKHGgt7LVGeTPIhzNjT7yEc_X47mVca4lLuT8w9kU-nvGjx0W4l6az9Ku9IWTGq0PR72_qQc2g=w2400)


# Hypothesis Testing

P-value, or critical value $\alpha$ of hypothesis testing of a model shows probability that the correlation happend just on chance. <br>
Typically in social sciences, we accept our hypothesis if p-value is less than 0.1, 0.05, or 0.01.

We will use `ttest_ind`. <br>
We can use this test, if we observe two independent samples from the same or different population, e.g. exam scores of boys and girls or of two ethnic groups. The test measures whether the average (expected) value differs significantly across samples. If we observe a large p-value, for example larger than 0.05 or 0.1, then we cannot reject the null hypothesis of identical average scores. If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%, then we reject the null hypothesis of equal averages.


```python
df = pd.read_csv('grades.csv')
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>student_id</th>
      <th>assignment1_grade</th>
      <th>assignment1_submission</th>
      <th>assignment2_grade</th>
      <th>assignment2_submission</th>
      <th>assignment3_grade</th>
      <th>assignment3_submission</th>
      <th>assignment4_grade</th>
      <th>assignment4_submission</th>
      <th>assignment5_grade</th>
      <th>assignment5_submission</th>
      <th>assignment6_grade</th>
      <th>assignment6_submission</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>B73F2C11-70F0-E37D-8B10-1D20AFED50B1</td>
      <td>92.733946</td>
      <td>2015-11-02 06:55:34.282000000</td>
      <td>83.030552</td>
      <td>2015-11-09 02:22:58.938000000</td>
      <td>67.164441</td>
      <td>2015-11-12 08:58:33.998000000</td>
      <td>53.011553</td>
      <td>2015-11-16 01:21:24.663000000</td>
      <td>47.710398</td>
      <td>2015-11-20 13:24:59.692000000</td>
      <td>38.168318</td>
      <td>2015-11-22 18:31:15.934000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1</td>
      <td>86.790821</td>
      <td>2015-11-29 14:57:44.429000000</td>
      <td>86.290821</td>
      <td>2015-12-06 17:41:18.449000000</td>
      <td>69.772657</td>
      <td>2015-12-10 08:54:55.904000000</td>
      <td>55.098125</td>
      <td>2015-12-13 17:32:30.941000000</td>
      <td>49.588313</td>
      <td>2015-12-19 23:26:39.285000000</td>
      <td>44.629482</td>
      <td>2015-12-21 17:07:24.275000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>D0F62040-CEB0-904C-F563-2F8620916C4E</td>
      <td>85.512541</td>
      <td>2016-01-09 05:36:02.389000000</td>
      <td>85.512541</td>
      <td>2016-01-09 06:39:44.416000000</td>
      <td>68.410033</td>
      <td>2016-01-15 20:22:45.882000000</td>
      <td>54.728026</td>
      <td>2016-01-11 12:41:50.749000000</td>
      <td>49.255224</td>
      <td>2016-01-11 17:31:12.489000000</td>
      <td>44.329701</td>
      <td>2016-01-17 16:24:42.765000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>FFDF2B2C-F514-EF7F-6538-A6A53518E9DC</td>
      <td>86.030665</td>
      <td>2016-04-30 06:50:39.801000000</td>
      <td>68.824532</td>
      <td>2016-04-30 17:20:38.727000000</td>
      <td>61.942079</td>
      <td>2016-05-12 07:47:16.326000000</td>
      <td>49.553663</td>
      <td>2016-05-07 16:09:20.485000000</td>
      <td>49.553663</td>
      <td>2016-05-24 12:51:18.016000000</td>
      <td>44.598297</td>
      <td>2016-05-26 08:09:12.058000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5ECBEEB6-F1CE-80AE-3164-E45E99473FB4</td>
      <td>64.813800</td>
      <td>2015-12-13 17:06:10.750000000</td>
      <td>51.491040</td>
      <td>2015-12-14 12:25:12.056000000</td>
      <td>41.932832</td>
      <td>2015-12-29 14:25:22.594000000</td>
      <td>36.929549</td>
      <td>2015-12-28 01:29:55.901000000</td>
      <td>33.236594</td>
      <td>2015-12-29 14:46:06.628000000</td>
      <td>33.236594</td>
      <td>2016-01-05 01:06:59.546000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
len(df)
```




    2315




```python
early = df[df['assignment1_submission'] <= '2015-12-31']
late = df[df['assignment1_submission'] > '2015-12-31']
```


```python
early.mean()
```




    assignment1_grade    74.972741
    assignment2_grade    67.252190
    assignment3_grade    61.129050
    assignment4_grade    54.157620
    assignment5_grade    48.634643
    assignment6_grade    43.838980
    dtype: float64




```python
late.mean()
```




    assignment1_grade    74.017429
    assignment2_grade    66.370822
    assignment3_grade    60.023244
    assignment4_grade    54.058138
    assignment5_grade    48.599402
    assignment6_grade    43.844384
    dtype: float64




```python
from scipy import stats
```


```python
stats.ttest_ind(early['assignment1_grade'], late['assignment1_grade'])
```




    Ttest_indResult(statistic=1.400549944897566, pvalue=0.16148283016060577)




```python
stats.ttest_ind(early['assignment2_grade'], late['assignment2_grade'])
```




    Ttest_indResult(statistic=1.3239868220912567, pvalue=0.18563824610067967)




```python
stats.ttest_ind(early['assignment3_grade'], late['assignment3_grade'])
```




    Ttest_indResult(statistic=1.7116160037010733, pvalue=0.08710151634155668)



According to `ttest_ind`, mean value of assignment3 grade between early students and late students might be correlated.

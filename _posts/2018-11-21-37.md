---
title: Bidirectional RNN and Deep RNN
categories: Deep-Learning Recurrent-Neural-Networks
header:
  teaser: /assets/teasers/7.jpg
---



Original Source: https://www.coursera.org/specializations/deep-learning



# Bidirectional RNN

Let's say that we want to do name entity recognition. Following two examples are included in the training set; 'He said, "Teddy bears are on sale!"', and 'He said, "Teddy Roosevelt was a great President!"'.

If the information flows only from left to right, our model has no way to learn that "Teddy" in the second example is a name.

To solve this kind of problem, bidirectional RNN (BRNN) is introduced.

In BRNN, there are two forward propagations; left to right and right to left. For each layer we combine activations from these two forward props to output prediction.

For each sequential layer, output is computed as following;

$$
\hat{y}^{<t>} = g(W_y[\overrightarrow{a}^{<t>}, \overleftarrow{a}^{<t>}] + b_y)
$$


![brnn](https://lh3.googleusercontent.com/iIrosdlSLxNvXyvuF_7idrI-hZd7AwBeoflhenkIkX8G-lTGjW0-2XhkQsO7Kr5hsXmmC1I3Y5pDpj8vC8ELqZSH5MgqSSXrPnHBu7lhFsQZAjMQ7-X41iMtIiFry3tpZ9iO_JDOVw=w2400)

In natural language processing, BRNN with LSTM is the most standard model.

# Deep RNN

We can make the RNN model more complex by adding layers vertically.

![Deep RNN](https://lh3.googleusercontent.com/GuCkshGnfjxpGMo1-S1XB3o0nGxGN9fuZ-lj8xKMp6UY6zaf0XwIkFVsvxltpEJgCPyiFlzPl04j9FqQRt8HOhrY_MxNSr_3GZM4uepGmKbU7QJpvO7ySlE_uo3tirBxFPsgVEA_Pg=w2400)

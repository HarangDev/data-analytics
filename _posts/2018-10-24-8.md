---
title: Mini-Batch Gradient Descent
categories: Deep-Learning
header:
  teaser: /assets/teasers/7.jpg
---

We call set of all of our training examples 'batch'. When we train batch once, we say we've trained for one 'epoch'.

When number of examples become large, using a batch to perform one gradient descent is computationally expensive and takes a lot of time.

To solve this problem, we divide our batch into many 'mini-batches', and use a mini-batch to perform one gradient descent; in other words, perform many gradient descents per one epoch.

When mini-batch size is 1, we call it 'stochastic gradient descent'. We use one example to perform one gradient descent, so we perform $m$ gradient desents per one epoch.

Stochastic gradient descent doesn't make use of speed that comes out of vectorization, and batch gradient descent is too slow as it performs one gradient descent per epoch. Thus in practice, we use mini-batch gradient descent with mini-batch size of 64, 128, 256, 512.

# Cost Curve of mini-batch gradient descent.

Unlike batch gradient descent, cost of mini-batch gradient descent doesn't always decrease after one gradient descent. That's because cost of mini-batch gradient is calculated for every mini-batch and there can be easy-to-fit mini-batch and hard-to-fit mini-batch.

![cost of mini-batch gradient descent](https://lh3.googleusercontent.com/BD3wc8dtHxlHxNl7HNzEmQ0G2WkyIPBGml7fqDxROfLnFN9tK-MXEtoOMXF7iuGiuCXuf91S8rmCAt-yJivHE59mKZvks5ZgBKREMI3aGCAO1kGc2CXcom7axyyNJLlZVz2zLx_81g=w2400)

Below is contour line of cost with respect to 2-dimensional features. Colored lines indicate gradient descents of different gradient descent types. While batch gradient descent converges straightly to local minimum, batch and stochastic gradient descent oscillates.

![cost of mini-batch gradient descent](https://lh3.googleusercontent.com/qvb5JD8A7HCJF3wyfy-L9bh7b5EXOX3LzfAxmxpxJxsj43w3z0m-MnFX5xpSrQICXt-YTHX1R_NkorSyNdYXNQky7AACPIAvy2Ob9QLQ5Kl6LAKmPx5jK8ImZ95nA91gCC7KVbwW3g=w2400)

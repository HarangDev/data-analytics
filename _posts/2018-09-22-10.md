---
title: Neural Networks
categories: Machine-Learning
header:
  teaser: /assets/teasers/6.jpg
---

# Why Neural Networks?

We have used linear regression and logistic regression with polynomial features to represent complex relationships between input features and output. However, it was computationally inefficient, since number of features grows rapidly as we use higher polynomial degrees.

Neural networks is an alternative way and also a popular way to represent complex relationships.

# What is Neural Networks?

![neural networks](https://lh3.googleusercontent.com/YqtV9cUDq8pA8F2keNh-iRfm_pRWFYZZWGO46MpKKSZqke8SxRTUyeoosP9rQ1Okhj8_V7Dw-VAww5VLW47HYoAkX8cHlewqSVNNxDdwY8Mu6gL4v27AlXwKhEkUuwP_wtexTi7B0A=w2400)

Neural networks is basically a stack of linear regressions. More precisely, it is a stack of layers.

In linear and logistic regression, we had only two layers; input layer and output layer. In neural networks, we have input layer, output layer, and hidden layers.

Also, in neural networks output $Z$ of each hidden layer is fed into a function called **activation function** and transforms into a matrix $A$ composed of **activation units** which is fed as an input of next layer. This activation function is applied element-wise.

This activation function is applied to add nonlinearity to the network.

# Activations

There are many possible activation functions, including sigmoid function which we've seen.

![activation functions](https://lh3.googleusercontent.com/he2WkslTRVYOMpuHUwfe_8YVvQdHJ8Tps806yUri2GIksp84V2WUAduYDe3-whBy_6nn_mecCWMfpJdd_iQY9EoZ7PyDPQnQ1OyTMCDZr3lVghQ5t4iam68K1bafXpOzbHVu3dMTag=w2400)

The most common one is ReLu function.

# Architecture

## Notations

$m$: number of training examples

$n_i$: number of features of ith layer. ($n_0$ is the number of input features)

$X$: input matrix; $m$ by $n_0$

$\Theta_i$: the weights we should optimize; $m$ by $n_i$

$Z_i$: output of linear function; $m$ by $n_i$

$A_i$: output of activation function; $m$ by $n_i$

This is an example of a neural network with two hidden layers.
(In this example, $A_1$ is a matrix. It doesn't mean anything like column vector of matrix $A$)

### Input Layer

X

### First Hidden Layer

$$ Z_1 = X\Theta_0 $$

$$ A_1 = g(Z_1) $$

### Second Hidden Layer

$$ Z_2 = A_1\Theta_1 $$

$$ A_2 = g(Z_2) $$

### Ouput Layer

for regression

$$ \hat{Y}=A_2\Theta_2$$

for binary classification

$$ \hat{Y}=sigmoid(A_2\Theta_2) $$

for multiclass classification

$$ \hat{Y}=softmax(A_2\Theta_2) $$

# Cost

For non-regularized regression and classification, cost function of neural networks is the same as the cost function we've seen before.

But with regularized regression and classification, the regularization term we add is slightly different since we have more $\Theta$ in nn. We add all the squares of scalar $\theta$s, multiply it with $\frac{\lambda}{2m}$ and add it to our original cost function.

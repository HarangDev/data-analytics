---
title: Neural Networks
categories: Machine-Learning
header:
  teaser: /assets/teasers/6.jpg
---

# Why Neural Networks?

We have used linear regression and logistic regression with polynomial features to represent complex relationships between input features and output. However, it was computationally inefficient, since number of features grows rapidly as we use higher polynomial degrees.

Neural networks is an alternative way and also a popular way to represent complex relationships.

# What is Neural Networks?

![neural networks](https://lh3.googleusercontent.com/YqtV9cUDq8pA8F2keNh-iRfm_pRWFYZZWGO46MpKKSZqke8SxRTUyeoosP9rQ1Okhj8_V7Dw-VAww5VLW47HYoAkX8cHlewqSVNNxDdwY8Mu6gL4v27AlXwKhEkUuwP_wtexTi7B0A=w2400)

Neural networks is basically a stack of linear regressions. More precisely, it is a stack of layers.

In linear and logistic regression, we had only two layers; input layer and output layer. In neural networks, we have input layer, output layer, and hidden layers.

Also, in neural networks output $Z$ of each hidden layer is fed into a function called **activation function** and transforms into a matrix $A$ composed of **activation units** which is fed as an input of next layer. This activation function is applied element-wise.

This activation function is applied to add nonlinearity to the network.

# Architecture

## Notations

$m$: number of training examples

$n_i$: number of features of ith layer. ($n_0$ is the number of input features)

$X$: input matrix; $m$ by $n_0$

$\Theta_i$: the weights we should optimize; $m$ by $n_i$

$Z_i$: output of linear function; $m$ by $n_i$

$A_i$: output of activation function; $m$ by $n_i$

This is an example of a neural network with two hidden layers.
(In this example, $A_1$ is a matrix. It doesn't mean anything like column vector of matrix $A$)

Input Layer: X

First Hidden Layer
$$ Z_1 = X\Theta_0 $$
$$ A_1 = g(Z_1) $$

Second Hidden Layer
$$ Z_2 = A_1\Theta_1 $$
$$ A_2 = g(Z_2) $$

Ouput Layer
1. for regression
$$ \hat{Y}=A_2\Theta_2$$
2. for binary classification
$$ \hat{Y}=sigmoid(A_2\Theta_2) $$
3. for multiclass classification
$$ \hat{Y}=softmax(A_2\Theta_2) $$

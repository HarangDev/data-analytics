---
title: Autoencoders
categories: Tips
header:
  teaser: /assets/teasers/9.jpg
---

https://youtu.be/o_peo6U7IRM



## Gradient Descent

Approximation by Taylor Expansion


$$
L(\theta + \Delta\theta) \approx L(\theta)+\nabla L\cdot\Delta\theta
$$

$$
\Delta L = \nabla L\cdot\Delta\theta
$$


If we set


$$
\Delta \theta = -\eta\nabla L
$$


then


$$
\Delta L = -\eta ||\nabla L||^2
$$


which is negative where $\eta > 0$ and $\nabla L != 0$. ($\eta$ is learning rate)

So if we update $\theta$ with $\theta - \eta \nabla L$, the loss decreases. But note that it only applies when $\eta$ is sufficiently small, since first order taylor approximation is valid only locally.
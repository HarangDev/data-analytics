---
title: Autoencoders
categories: Tips
header:
  teaser: /assets/teasers/9.jpg
---



Original Source: https://youtu.be/o_peo6U7IRM



## Gradient Descent

Approximation by Taylor Expansion


$$
L(\theta + \Delta\theta) \approx L(\theta)+\nabla L\cdot\Delta\theta
$$

$$
\Delta L = \nabla L\cdot\Delta\theta
$$


If we set


$$
\Delta \theta = -\eta\nabla L
$$


then


$$
\Delta L = -\eta ||\nabla L||^2
$$


which is negative where $\eta > 0$ and $\nabla L != 0$. ($\eta$ is learning rate)

So if we update $\theta$ with $\theta - \eta \nabla L$, the loss decreases. But note that it only applies when $\eta$ is sufficiently small, since first order taylor approximation is valid only locally.



## BackPropagation Algorithm



* $C$: cost function
* $L$: last layer
* $l$: layer index
* $\delta^l$: error term at $l$th layer
* $\sigma$: activation function
* $z^l$: output of layer $l$
* $a^l$: activation of $z^l$
* $W^l$: weight of layer $l$
* $b^l$: bias of layer $l$


$$
\delta ^L = \nabla_aC \odot \sigma'(z^L)
$$

$$
\delta^l = \sigma'(z^l) \odot ((w^{l+1})^T \delta^{l+1})
$$

$$
\nabla_{b^l}C = \delta^l
$$

$$
\nabla_{W^l}C = \delta^l(a^{l-1})^T
$$



## Sigmoid vs ReLU

Sigmoid function is formulated:


$$
sigmoid(x) = \frac{1}{1+e^{-x}}
$$


![]()



Maximum $sigmoid ' (x)$ is 0.25, and if $x$ goes far from 0,  $sigmoid ' (x)$ approaches 0. Therefore, if we use sigmoid as activation function, the error signal gets smaller and smaller as we backpropagate. So the parameters at the front part of the network do not get updated well. This is called vanishing gradient problem.

On the other hand, derivative of ReLU is 1 for $x>0$ and 0 for $x<0$, so it does not create vanishing gradient.

Also, for classification, if we use cross entropy for loss function, $\delta^L$ becomes $\hat{y} - y$, so the problem of derivative of sigmoid disappears in the last layer.



## Maximum Likelihood Estimation in Cost Function



We assume the target distribution given network output follows known distribution parameterized by network output. We assume target $Y$ follows below distribution


$$
p(Y|f_\theta(X))
$$


When we observe input $x_i$ and target $y_i$, its likelihood is:


$$
p(y_i|f_\theta(x_i))
$$


We observe multiple instances, and with i.i.d condition, the likelihood of observing input vector $x$ and target vector $y$ is:


$$
\prod_i p(y_i|f_\theta(x_i))
$$


and we can do maximum likelihood estimation by minimizing negative log likelihood:


$$
-\sum_i\log(p(y_i|f_\theta(x_i)))
$$


We'll see how this negative log likelihood formulates in two distributions.

### 1. Gaussian Distribution -> MSE

There are two parameters in Gaussian distribution: mean and standard deviation. We assume $\sigma = 1$. We set  network output as the mean of the Gaussian distribution. So we are interpreting the network output is the mean of the target distribution given $x$.


$$
f_\theta(x_i) = \mu_i
$$


We want to minimize the negative log likelihood, which, if we do some calculations, is:


$$
-\sum_i\log(p(y_i|f_\theta(x_i))) = -n\log\frac{1}{\sqrt{2\pi}} + \sum_i\frac{(y_i-f_\theta(x_i))^2}{2}
$$


Minimizing above term with respect to $\theta$ is the same as minimizing the following term:


$$
\frac{1}{n}\sum_i(y_i-f_\theta(x_i))^2
$$


which is Mean Squared Error.

### 2. Bernoulli Distribution -> BCE

We interpret the network output is the $p$ parameter of the Bernoulli distribution.


$$
f_\theta(x_i) = p_i
$$

$$
-\sum_i\log(p(y_i|f_\theta(x_i))) = -\sum_i [y_i\log f_\theta(x_i) + (1-y_i)\log(1-f_\theta(x_i))]
$$


which is Binary Cross Entropy
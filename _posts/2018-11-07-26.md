---
title: ResNet
categories: Deep-Learning Convolutional-Neural-Networks
header:
  teaser: /assets/teasers/7.jpg
---



Original Source: https://www.coursera.org/specializations/deep-learning



*He et al., 2015. Deep residual networks for image recognition*

# Architecture

![residual block](https://lh3.googleusercontent.com/gEhIHAGBWEVK_WJ2xdDItxUzKlZhJLHHUnwMgc5EyBG8N3GKG_ZiIN7ZzciMp6xxFdL1KKbYPvn4uUizuL7hNU4SJk-dzVJBAGgSAfPvaVfGrmovuXMn_FhJL1vGrJf25vBlZRbkWg=w2400)

ResNet is composed of many residual blocks that look like above.

$$
a^{[l+2]} = g(z^{[l+2]} + a^{[l]})
$$

The first network in the below image is an example of ResNet.

![ResNet](https://lh3.googleusercontent.com/kyy5XksBNE6okiU9Wk4UcBHBcRAkIBc3lObWMxUmTxNYk6oecO2fUL9TYpU6tk-1n5H12AWCwn2FTxq10oJbI0wg0gmDoBLJ9PnT9P23FJ7SLBPnJy__P7XM5ECz2Zz7D1vOUY3E_g=w2400)

# Why ResNet?

When plain neural network becomes too deep, its error increases in reality, unlike in theory.

![resnet vs plain](https://lh3.googleusercontent.com/sDJ9qJPeMMcIcX7GR--10_0MwyDOyjIlXq50AGj67VezMJQ2-3TeN7fhy3GVbh-XCLgusuSh5-ge-mjEKSK8luTq8qwr7YFi-Ru5ea0lWxt_nJUsD4ViTJYTMzDm4_1agZlvFejC8Q=w2400)

ResNets perform well with deep networks. But why?

It's because identity function is easy for residual blocks to learn.

$$
a^{[l+2]} = g(W^{[l+1]}a^{[l+1]} + b^{[l+1]} + a^{[l]})
$$

When $W$ and $b$ is close to 0, $a^{[l+2]}=a^{[l]}$.

So adding residual block to somewhere in a neural network at least doesn't hurt its performance but only can improve performance.

# Keeping Dimensionalities

To accomplish $a^{[l+2]} = g(W^{[l+1]}a^{[l+1]} + b^{[l+1]} + a^{[l]})$, dimensionalities of $a^{[l]}$ and $a^{[l+2]}$ should be same.

So we mostly use same convolutions. If the dimensionalities don't match, we multiply $W_s$ to $a^{[l]}$.
$$
a^{[l+2]} = g(W^{[l+1]}a^{[l+1]} + b^{[l+1]} + W_sa^{[l]})
$$
For example when shape of $a^{[l+2]}$ is 256 and shape of $a^{[l]}$ is 128, $W_s$ should be a weight matrix of 256x128.

---
title: Optimistic Initial Values
categories: Reinforcement-Learning
header:
  teaser: /assets/teasers/8.jpg
---

Optimistic initial values is another simple way of solving the explore-exploit dilemma.

In epsilon-greedy method, we've set initial values to 0. However in optimistic initial values method, we set initial mean estimates of the rewards of bandit machines to a ceiling. A ceiling is a value we are sure that each bandit machine's reward falls below.

This initial sample mean is 'too good to be true', and all collected data will cause it to go down. If the true mean is 1, the sample mean will eventually settle into 1.

If you haven't explored a bandit, its mean will remain high, causing you to explore it more. Thus we don't need epsilon with this method; we only need greedy strategy.


```python
import numpy as np
import matplotlib.pyplot as plt
```


```python
class Bandit:

    def __init__(self, m):
        self.m = m
        # set initial reward mean to a ceiling (we know that every bandit reward is lower than 10)
        self.mean = 10
        self.N = 0

    def pull(self):
        return np.random.randn() + self.m

    def update(self, x):
        self.N += 1
        self.mean = (1-1/self.N)*self.mean + 1/self.N*x
```


```python
def run_experiment(m1, m2, m3, N):
    bandits = [Bandit(m1), Bandit(m2), Bandit(m3)]
    data = np.empty(N)
    for i in range(N):
        p = np.random.random()
        j = np.argmax([bandit.mean for bandit in bandits])
        x = bandits[j].pull()
        bandits[j].update(x)
        data[i] = x
    cumulative_average = np.cumsum(data) / (np.arange(N)+1)

    print('mean reward for each choice after {} trials with optimistic initial values method'.format(N))
    print([b.mean for b in bandits])

    return cumulative_average
```


```python
# for comparison between epsilon-greedy method and optimistic-initial-values method

class BanditEps:

    def __init__(self, m):
        self.m = m
        self.mean = 0
        self.N = 0

    def pull(self):
        return np.random.randn() + self.m

    def update(self, x):
        self.N += 1
        self.mean = (1-1/self.N)*self.mean + 1/self.N*x

def run_experiment_eps(m1, m2, m3, eps, N):
    bandits = [BanditEps(m1), BanditEps(m2), BanditEps(m3)]
    data = np.empty(N)
    for i in range(N):
        p = np.random.random()
        if p < eps:
            j = np.random.choice(3)
        else:
            j = np.argmax([bandit.mean for bandit in bandits])
        x = bandits[j].pull()
        bandits[j].update(x)
        data[i] = x
    cumulative_average = np.cumsum(data) / (np.arange(N)+1)

    print('mean reward for each choice after {} trials with epsilon={}'.format(N, eps))
    print([b.mean for b in bandits])

    return cumulative_average
```


```python
if __name__ == '__main__':

    c_1 = run_experiment_eps(1.0, 2.0, 3.0, 0.1, 100000)
    oiv = run_experiment(1.0, 2.0, 3.0, 100000)

    # log scale plot
    plt.figure(figsize=(20,10))
    plt.plot(c_1, label='eps = 0.1')
    plt.plot(oiv, label='optimistic')
    plt.legend()
    plt.xscale('log')
    plt.show()
```

    mean reward for each choice after 100000 trials with epsilon=0.1
    [1.031402810149631, 2.007312325643326, 2.997453024884899]
    mean reward for each choice after 100000 trials with optimistic initial values method
    [1.8296230777106393, 2.3211802131371595, 3.0077036149035585]



![png](https://lh3.googleusercontent.com/X2dpxWfi19hdBOZEV9PZOxiT5J0Rwwr1xlrZyBkvXywcKYhDwyczop-pS9oeHp_y35qhfZFMm3DISnlHF7sAsKkVcTqLHdpO1SoEBgvSzlyFlR4SiqcZZ_WlBxmbfVMphICwTIcQsQ=w2400)


We can see from the above graph that optimistic initial values adapt to the best bandit machine more quickly.
